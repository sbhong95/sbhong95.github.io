---
title: "Deep Learning for Computer Vision Lec 4"
excerpt: "Regularization", "Optimization"
categories:
 - Deep Learning
tag:
 - Gradient Descent
 - Loss Function
use_math: true
toc: true
toc_sticky: true
---


# 4 : Regularization, Optimization


## Regularization : Beyond training error

Loss function : Data Loss + Regularization 

$$
L(w) = \frac{1}{n} \sum_{i=1}^N L_i(f(x_i, W), y_i) + \lambda R(W)
$$

- Data Loss : make model prediction should match training data.
- Regularization : prevent the model from doing too well on training data.

Examples 

- L1, L2 regularization
- Dropout
- Batch normalization
- Cutout, Mixup, Stochastic depth, …

Make model simpler…

## Optimizaing $w$

Goal : find weights that minimizes loss function

Methods based on gradient

- Numerical way : Approximate, fast, easy to apply.
- Analytic way : exact, slow, error - plane.

1. Gradient Descent - the basic method
    
    Update formula
    
    $$
    w_{new} = w_{old} - \alpha \nabla L(w_{old})
    $$
    
    Hyperparameters : 
    
    - Initial value of weights.
    - Number of steps.
    - Learning rate $\alpha$.

1. Stochastic Gradient Descent - if data is too large??? 
    
    Approximate expectation as sample mean.
    
    $$
    E_{XY \sim P}[L(x, y, W)] \approx \frac 1N \sum_{i-1}^N L(x_i, y_i, W) 
    $$
    
    Update formula
    
    $$
    w_{new} = w_{old} -\alpha\nabla L(w_{old})
    $$
    
    where $\nabla L$ is computed by minibatch
    
    Hyperparameters :
    
    - Initial value of weights
    - Number of steps
    - Learning rate $\alpha$
    - Batch size
    - How to sample batch
    
    Problem : 
    
    - If zero gradient like local minima or saddle point?
        
        → get stuck…
        

1. SGD + Momentum 
    
    Way to overcome zero gradient problem.
    
    Add momentum term to gradient. → length of each step is longer than SGD.
    
    Update formula
    
    $$
    \begin{align*} v_{new} &= \rho v_{old} - \alpha \nabla f(w_{old})\\ w_{new} &= w_{old} + v_{new}\end{align*}
    $$
    
    Can overcome local minima or saddle point
    
    Step size become longer → fast convergence.
    
2. Nesterov Momentum
    
    Different way of adding momentum
    
    Update formula 
    
    $$
    \begin{align*}v_{new} &= \rho v_{old} - \alpha \nabla f(w_{old} + \rho v_{old})\\w_{new} &= w_{old} + v_{new}\end{align*}
    $$
    
    $\nabla f(w_{old} + \rho v_{old})$ ← not the part what we want 
    
    Thus reparameterizing… $\tilde w = w + \rho v$.
    
    $$
    \begin{align*}v_{new} &= \rho v_{old} - \alpha \nabla f(\tilde w_{old})\\ \tilde w_{new} &= \tilde w_{old} + v_{new} + \rho(v_{new} - v_{old})\end{align*}
    $$
    

1. Adagrad - adaptive learning rates
    
    Element-wise scaling
    
    $$
    \begin{align*}v_{new} &= v_{old} + \nabla f(w_{old}) * \nabla f(w_{old})\\w_{new} &= w_{old} - \alpha \nabla f(w_{old}) / (\sqrt{ v_{new}} + \varepsilon) \end{align*}
    $$
    
    $*$ : element-wise product
    
    $\varepsilon$ : value close to zero.
    

1. RMSProp
    
    Linear combination of $v_{old}$ and $\nabla f(w_{old})$.
    
    $$
    \begin{align*}v_{new} &= \beta \cdot v_{old} + (1 - \beta)\cdot \nabla f(w_{old}) * \nabla f(w_{old})\\w_{new} &= w_{old} - \alpha \nabla f(w_{old}) / (\sqrt{ v_{new}} + \varepsilon) \end{align*}
    $$
    
2. Adam RMSProp + Momentum
    
    Initial value : $m_1 = m_2 = 0$
    
    $$
    \begin{align*}m_1 &= \beta_1  m_1 + (1 - \beta_1)\nabla f(w_{old})\\m_2 &=\beta_2m_2 + (1 - \beta_2)\nabla f(w_{old}) * \nabla f(w_{old})\\ w_{new} &= w_{old} - \alpha m_1 / (\sqrt{m_2} + \epsilon)\end{align*}
    $$
    

Default : Adam!

Newton method → require hessian matrix and inversion → high computational cost.

However, L-BFGS can be a way if you can afford to do full batch…